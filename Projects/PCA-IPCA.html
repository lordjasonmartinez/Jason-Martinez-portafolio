<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>KPCA</title>
    <link rel="stylesheet" href="../style.css">
    <link rel="stylesheet" href="https://cdn.lineicons.com/2.0/LineIcons.css">
    <script src="https://d3js.org/d3.v6.min.js"></script>
</head>

<body>
    <header>
        <div class="hamburguesa">&#9776;</div>
        <nav>
            <a href="../index.html">Home</a>
            <a href="#Analisis">PCA y IPCA</a>
            <a href="#Procesamiento">ETL</a>
            <a href="#Conclusiones">Conclusiones estrat√©gicas</a>
            <a href="#Repositorio">Repositorio</a>
        </nav>
    </header>

<script src="../script.js"></script>

    <main>
        <section id="Analisis" class="proyectos">
            <h2>An√°lisis PCA y IPCA: Descubriendo Patrones Ocultos en Comportamiento del Consumidor</h2>
            <p>Los modelos de regresi√≥n log√≠stica entrenados con PCA e IPCA obtuvieron una exactitud de 0.625 y 0.62167 respectivamente. Esto significa que ambos modelos fueron capaces de predecir correctamente el resultado aproximadamente el 62% de las veces.</p>
            <h4>An√°lisis de los Modelos</h4>
            <h4>PCA</h4>
            <p>El An√°lisis de Componentes Principales (PCA) es una t√©cnica de reducci√≥n de dimensionalidad que se utiliza para transformar un conjunto de datos de alta dimensionalidad en un conjunto de datos de baja dimensionalidad. En este caso, se utiliz√≥ PCA para reducir el conjunto de datos a tres componentes principales.</p>
            <p>El modelo de regresi√≥n log√≠stica entrenado con PCA obtuvo una exactitud de 0.625. Esto indica que el modelo fue capaz de predecir correctamente el resultado el 62.5% de las veces.</p>
            <h4>IPCA</h4>
            <p>El An√°lisis de Componentes Principales Incremental (IPCA) es una variante de PCA que se utiliza cuando los datos son demasiado grandes para caber en la memoria. IPCA divide los datos en lotes m√°s peque√±os y los procesa de forma incremental.</p>
            <p>El modelo de regresi√≥n log√≠stica entrenado con IPCA obtuvo una exactitud de 0.62167. Esto indica que el modelo fue capaz de predecir correctamente el resultado el 62.167% de las veces.</p>
            <img src="../img/cpa-ipca/pca-ipca.png" class="grafico">
        </section>

        <section id="Procesamiento" class="proyectos">
            <h2>Procesamiento de Datos</h2>
            <h3>Procesamiento de Datos con PCA</h3>
            <p>1. Se eliminaron las columnas ‚Äòreordered‚Äô, ‚Äòdepartment‚Äô y ‚Äòproduct_name‚Äô del conjunto de datos.</p>
            <p>2. Se estandarizaron las caracter√≠sticas utilizando StandardScaler().</p>
            <img src="../img/cpa-ipca/pca_1.png" class="grafico">
            <p>3. Se dividi√≥ el conjunto de datos en conjuntos de entrenamiento y prueba utilizando train_test_split().</p>
            <p>4. Se manejaron los valores faltantes (NaN) en los conjuntos de entrenamiento utilizando SimpleImputer(strategy='mean').</p>
            <img src="../img/cpa-ipca/pca_2.png" class="grafico">
            <p>5. Se aplic√≥ PCA a los conjuntos de entrenamiento sin valores faltantes.</p>
            <p>6. Se manejaron los valores faltantes (NaN) en los conjuntos de prueba utilizando el mismo SimpleImputer que se ajust√≥ en el conjunto de entrenamiento.</p>
            <img src="../img/cpa-ipca/pca_3.png" class="grafico">
            <p>7. Se aplic√≥ PCA a los conjuntos de prueba sin valores faltantes utilizando el mismo PCA que se ajust√≥ en el conjunto de entrenamiento.</p>
            <h3>Procesamiento de Datos con IPCA</h3>
            <p>1. Eliminaci√≥n de columnas: Se eliminaron las columnas ‚Äòreordered‚Äô, ‚Äòdepartment‚Äô y ‚Äòproduct_name‚Äô del conjunto de datos. Estas columnas se consideraron irrelevantes para el modelo de regresi√≥n log√≠stica.</p>
            <p>2. Estandarizaci√≥n: Se estandarizaron las caracter√≠sticas utilizando StandardScaler(). La estandarizaci√≥n es un paso crucial en el preprocesamiento de datos ya que los modelos de aprendizaje autom√°tico suelen tener un mejor rendimiento cuando todas las caracter√≠sticas est√°n en la misma escala.</p>
            <p>3. Divisi√≥n de datos: Se dividi√≥ el conjunto de datos en conjuntos de entrenamiento y prueba utilizando train_test_split(). Esta es una pr√°ctica com√∫n en el aprendizaje supervisado para evaluar la capacidad del modelo para generalizar a datos no vistos.</p>
            <img src="../img/cpa-ipca/ipca_1.png" class="grafico">
            <p>4. Manejo de valores faltantes: Se manejaron los valores faltantes (NaN) en los conjuntos de entrenamiento utilizando SimpleImputer(strategy='mean'). Los valores faltantes pueden causar problemas con muchos modelos de aprendizaje autom√°tico, por lo que es importante manejarlos antes de entrenar el modelo.</p>
            <p>5. Aplicaci√≥n de IPCA: Se aplic√≥ IPCA a los conjuntos de entrenamiento sin valores faltantes. IPCA es una variante de PCA que se utiliza cuando los datos son demasiado grandes para caber en la memoria. IPCA divide los datos en lotes m√°s peque√±os y los procesa de forma incremental. Esto puede ser beneficioso cuando se trabaja con grandes conjuntos de datos ya que puede reducir el uso de memoria y acelerar el tiempo de procesamiento.</p>
            <p>6. Manejo de valores faltantes en los datos de prueba: Se manejaron los valores faltantes (NaN) en los conjuntos de prueba utilizando el mismo SimpleImputer que se ajust√≥ en el conjunto de entrenamiento. Es importante utilizar el mismo imputador para evitar la fuga de datos.</p>
            <p>7. Aplicaci√≥n de IPCA a los datos de prueba: Se aplic√≥ IPCA a los conjuntos de prueba sin valores faltantes utilizando el mismo IPCA que se ajust√≥ en el conjunto de entrenamiento. Al igual que con el imputador, es importante utilizar el mismo IPCA para evitar la fuga de datos.</p>
            <img src="../img/cpa-ipca/ipca_2.png" class="grafico">
            <p>Es importante recordar que tanto PCA como IPCA deben ser ajustados solo en el conjunto de entrenamiento para evitar la fuga de datos. Luego, estos ajustes se utilizan para transformar tanto el conjunto de entrenamiento como el de prueba. Esto asegura que el modelo se eval√∫a de manera justa en datos no vistos.</p>
        </section>

        <section id="Conclusiones" class="proyectos">
            <h2>Conclusiones:</h2>
            <p>Ambos modelos, PCA e IPCA, obtuvieron resultados similares en t√©rminos de exactitud. Sin embargo, la elecci√≥n entre PCA e IPCA depender√° del tama√±o de los datos y de las limitaciones de memoria. Si los datos son demasiado grandes para caber en la memoria, IPCA puede ser una mejor opci√≥n.</p>
            <p>La gr√°fica de la varianza explicada muestra cu√°nta informaci√≥n (varianza) puede ser atribuida a cada uno de los componentes principales. Esto es √∫til para entender la importancia de cada componente y para decidir cu√°ntos componentes se deben mantener.</p>
            <p>Es importante recordar que aunque la exactitud es una medida √∫til de rendimiento, no es la √∫nica. Tambi√©n se deben considerar otras m√©tricas como la precisi√≥n, la exhaustividad y el √°rea bajo la curva ROC (AUC-ROC). Adem√°s, se debe realizar una validaci√≥n cruzada para asegurar que el modelo es robusto y generaliza bien a datos no vistos.</p>
            <p>Finalmente, es importante tener en cuenta que estos resultados son espec√≠ficos para este conjunto de datos y para estos modelos. Los resultados pueden variar con diferentes conjuntos de datos o con diferentes modelos. Por lo tanto, siempre es importante probar diferentes t√©cnicas y modelos para encontrar la mejor soluci√≥n para un problema espec√≠fico.</p>
        </section>

        <section id="Repositorio" class="proyectos">
            <h2>Repositorio</h2>
            <p>Si deseas validar c√≥mo se construy√≥ el algoritmo y la base de datos, puedes visitar el siguiente enlace. All√≠ encontrar√°s todos los detalles del c√≥digo y los datos utilizados en este proyecto.</p>
            <a href="https://github.com/Lordjasonmartinez/PCA-IPCA">üëâ Haz clic aqu√≠ üëà</a>
        </section>  
    </main>
</body>
</html>


